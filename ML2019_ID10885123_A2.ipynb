{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "ML2019_ID10885123_A2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlcurva77/Assignments/blob/master/ML2019_ID10885123_A2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47nF31JoWKLa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "07df7c75-1603-4bbb-dad4-4edf7d680f50"
      },
      "source": [
        "#Loading NLTK\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "!pip install -q nltk==3.4.3\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# spacy for lemmatization\n",
        "import spacy\n",
        "\n",
        "# Plotting tools\n",
        "!pip install pyLDAvis\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim  # don't skip this\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Enable logging for gensim - optional\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyLDAvis in /usr/local/lib/python3.6/dist-packages (2.1.2)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.10.1)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.7.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (3.6.4)\n",
            "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.3.1)\n",
            "Requirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.33.6)\n",
            "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.24.2)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.13)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.16.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (19.1.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (0.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.12.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.8.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (7.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (41.2.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.3.0)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2.5.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5YqZBDBYjfE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a03c07d4-81a6-4d6e-b5e9-7b4eaf8a1c4a"
      },
      "source": [
        "inGoogleColab = True\n",
        "folderpath_base = '/content/drive/My Drive/Docs'\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dqdXA60ZjLt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8f1e70d9-282e-40d8-89e2-6b6c35d53fc2"
      },
      "source": [
        "print(folderpath_base)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Docs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GzFKJEDWKLe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "14abf469-c2b6-4ae2-cbd8-8f5dafbc6126"
      },
      "source": [
        "import os\n",
        "from nltk.corpus import PlaintextCorpusReader\n",
        "\n",
        "#print(os.getcwd())\n",
        "#corpus_root = os.getcwd() + \"\\Docs\"\n",
        "#print (corpus_root)\n",
        "\n",
        "# .txt file names as file IDs\n",
        "file_ids = \".*.txt\"\n",
        "#corpus = nltk.corpus.reader.PlaintextCorpusReader(corpus_root, file_ids,encoding='latin-1')\n",
        "corpus = nltk.corpus.reader.PlaintextCorpusReader(folderpath_base, file_ids,encoding='latin-1')\n",
        "\n",
        "#print(corpus.fileids())\n",
        "print(corpus)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<PlaintextCorpusReader in '/content/drive/My Drive/Docs'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnucOTi_WKLi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "84bd9ed2-06d5-4fa5-bf60-452858372ac9"
      },
      "source": [
        "# for-loop through file IDs and print out word count. \n",
        "for f in corpus.fileids():\n",
        "    print(len(corpus.words(f)), f)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "713 01_AFR1.txt\n",
            "751 01_AFR2.txt\n",
            "1360 01_AFR3.txt\n",
            "906 01_AFR4.txt\n",
            "399 02_EY1.txt\n",
            "694 02_EY2.txt\n",
            "3577 02_EY3.txt\n",
            "12457 02_EY4.txt\n",
            "6403 02_EY5.txt\n",
            "7583 02_EY6.txt\n",
            "14264 02_EY7.txt\n",
            "821 02_EY8.txt\n",
            "1460 03_EY1.txt\n",
            "949 03_EY10.txt\n",
            "1305 03_EY2.txt\n",
            "1044 03_EY3.txt\n",
            "515 03_EY4.txt\n",
            "560 03_EY5.txt\n",
            "865 03_EY6.txt\n",
            "1371 03_EY7.txt\n",
            "1245 03_EY8.txt\n",
            "1201 03_EY9.txt\n",
            "3603 04_EY1.txt\n",
            "402 05_UNSW1.txt\n",
            "1345 05_UQ1.txt\n",
            "697 05_USYD1.txt\n",
            "578 05_UTS1.txt\n",
            "856 05_UWA1.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDmOW3EfWKLk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b9e45944-cefb-481b-f42c-01c306c64e37"
      },
      "source": [
        "# Corpus size in number of words\n",
        "print(len(corpus.words()))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "67924\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_Ga5yjdWKLn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2955ead5-99e8-4fc8-ce38-d7a4fdd1164e"
      },
      "source": [
        "#tokenized_sent = corpus.sents()\n",
        "#print(tokenized_sent)\n",
        "\n",
        "tokenized_sent = corpus.sents()\n",
        "print(tokenized_sent[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['US', 'business', 'leader', 'Kay', 'Koplovitz', 'forges', 'new', 'ground', 'for', 'female', 'entrepreneurs', 'by', 'Joanne', 'Gray', 'Women', 'in', 'science', 'and', 'technology', 'are', 'gaining', 'a', 'higher', 'profile', 'in', 'Australia', 'as', 'entrepreneurs', ',', 'says', 'Kay', 'Koplovitz', ',', 'a', 'leading', 'successful', 'businesswoman', 'in', 'the', 'US', 'and', 'founder', 'of', 'USA', 'Network', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1R_gMn58g6o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d4b3f32b-a479-48b5-a7c1-6d19664d0b30"
      },
      "source": [
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "\n",
        "data_words = list(sent_to_words(tokenized_sent))\n",
        "\n",
        "print(data_words[:2])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['us', 'business', 'leader', 'kay', 'koplovitz', 'forges', 'new', 'ground', 'for', 'female', 'entrepreneurs', 'by', 'joanne', 'gray', 'women', 'in', 'science', 'and', 'technology', 'are', 'gaining', 'higher', 'profile', 'in', 'australia', 'as', 'entrepreneurs', 'says', 'kay', 'koplovitz', 'leading', 'successful', 'businesswoman', 'in', 'the', 'us', 'and', 'founder', 'of', 'usa', 'network'], ['her', 'successful', 'incubator', 'springboard', 'enterprises', 'launched', 'in', 'australia', 'five', 'years', 'ago', 'trains', 'women', 'founders', 'with', 'potentially', 'high', 'growth', 'businesses', 'to', 'raise', 'venture', 'capital', 'with', 'week', 'course', 'and', 'two', 'day', 'boot', 'camp', 'which', 'runs', 'this', 'week']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckonue-E9Z6b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "77877c0c-7f92-4f78-f55f-5ed190be3433"
      },
      "source": [
        "# Build the bigram and trigram models\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "\n",
        "# Faster way to get a sentence clubbed as a trigram/bigram\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "\n",
        "# See trigram example\n",
        "print(trigram_mod[bigram_mod[data_words[1]]])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['her', 'successful', 'incubator', 'springboard', 'enterprises', 'launched', 'in', 'australia', 'five', 'years_ago', 'trains', 'women', 'founders', 'with', 'potentially', 'high', 'growth', 'businesses', 'to', 'raise', 'venture_capital', 'with', 'week', 'course', 'and', 'two', 'day', 'boot', 'camp', 'which', 'runs', 'this', 'week']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8OB2pUWWKLv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "37a734ff-d9d6-46f6-8145-421309a8e95c"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words=stopwords.words(\"english\")\n",
        "print(len(stop_words))\n",
        "\n",
        "stop_words.extend([\"can\", \"say\",\"one\",\"way\",\"use\",\"here\",\"I\",\n",
        "                 \"also\",\"howev\",\"tell\",\"will\",\"help\",\"also\",\n",
        "                 \"much\",\"need\",\"take\",\"tend\",\"even\",\"us\",\"US\"\n",
        "                 \"like\",\"particular\",\"rather\",\"said\",\"EY\",\"ey\"\n",
        "                 \"get\",\"well\",\"make\",\"ask\",\"come\",\"end\",\n",
        "                 \"first\",\"two\",\"help\",\"often\",\"may\",\n",
        "                 \"might\",\"see\",\"someth\",\"thing\",\"point\",\n",
        "                 \"post\",\"look\",\"right\",\"now\",\"think\",\"'ve \",\n",
        "                 \"'re \",\"anoth\",\"put\",\"set\",\"new\",\"good\",\n",
        "                 \"want\",\"sure\",\"kind\",\"larg\",\"yes,\",\"day\",\"etc\",\n",
        "                 \"quit\",\"sinc\",\"attempt\",\"lack\",\"seen\",\"awar\",\n",
        "                 \"littl\",\"ever\",\"moreov\",\"though\",\"found\",\"abl\",\n",
        "                 \"enough\",\"far\",\"earli\",\"away\",\"achiev\",\"draw\",\n",
        "                 \"last\",\"never\",\"brief\",\"bit\",\"entir\",\"brief\",\n",
        "                 \"great\",\"lot\",\"include\",\"provide\",\"year\",\"new\",\"help\",\n",
        "                 \"includ\",\"one\",\"require\",\"use\",\"make\",\"better\",\n",
        "                 \"way\",\"like\",\"across\",\"valu\",\"provide\",\"young\",\"continu\",\n",
        "                 \"tropfest\",\"ernst\",\"four\",\"limit\",\"and\",\"however\", \"\\x92\", \"\\x93\",\"\\x94\",\n",
        "                 \"\\x95\",\"\\x96\",\"\\x97\",\"\\x98\",\"\\x99\",\"the\",\"in\",\"because\",\"includ\",\"this\",\"those\",\"where\"])\n",
        "print(len(stop_words))\n",
        "print(stop_words)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "179\n",
            "303\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'can', 'say', 'one', 'way', 'use', 'here', 'I', 'also', 'howev', 'tell', 'will', 'help', 'also', 'much', 'need', 'take', 'tend', 'even', 'us', 'USlike', 'particular', 'rather', 'said', 'EY', 'eyget', 'well', 'make', 'ask', 'come', 'end', 'first', 'two', 'help', 'often', 'may', 'might', 'see', 'someth', 'thing', 'point', 'post', 'look', 'right', 'now', 'think', \"'ve \", \"'re \", 'anoth', 'put', 'set', 'new', 'good', 'want', 'sure', 'kind', 'larg', 'yes,', 'day', 'etc', 'quit', 'sinc', 'attempt', 'lack', 'seen', 'awar', 'littl', 'ever', 'moreov', 'though', 'found', 'abl', 'enough', 'far', 'earli', 'away', 'achiev', 'draw', 'last', 'never', 'brief', 'bit', 'entir', 'brief', 'great', 'lot', 'include', 'provide', 'year', 'new', 'help', 'includ', 'one', 'require', 'use', 'make', 'better', 'way', 'like', 'across', 'valu', 'provide', 'young', 'continu', 'tropfest', 'ernst', 'four', 'limit', 'and', 'however', '\\x92', '\\x93', '\\x94', '\\x95', '\\x96', '\\x97', '\\x98', '\\x99', 'the', 'in', 'because', 'includ', 'this', 'those', 'where']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzJLBf4Q9uXI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqWzJKNg-JlD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e6294119-3ccf-41c9-dfe9-82aaaec2203a"
      },
      "source": [
        "# Remove Stop Words\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Form Bigrams\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "# python3 -m spacy download en\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "\n",
        "# Do lemmatization keeping only noun, adj, vb, adv\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized[:1])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['business', 'leader', 'kay', 'koplovitz', 'forge', 'ground', 'female', 'entrepreneur', 'gray', 'woman', 'science', 'technology', 'gain', 'high', 'profile', 'australia', 'entrepreneur', 'say', 'kay', 'koplovitz', 'lead', 'successful', 'businesswoman', 'founder', 'usa', 'network']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1pNzHWJ-aXM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1decdea0-ae1b-4c29-8942-fe5ec2013b5c"
      },
      "source": [
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Create Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "# View\n",
        "print(corpus[:1])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[(0, 1), (1, 1), (2, 1), (3, 2), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 2), (12, 2), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1)]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDFGaAOr-fGd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b0fc9bd2-7608-4c32-8084-730f92b06c7d"
      },
      "source": [
        "id2word[1]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'business'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T57DfcPD-j3Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "d3e94f0f-7ff3-4b1f-ac2c-106d7da7dec8"
      },
      "source": [
        "# Human readable format of corpus (term-frequency)\n",
        "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('australia', 1),\n",
              "  ('business', 1),\n",
              "  ('businesswoman', 1),\n",
              "  ('entrepreneur', 2),\n",
              "  ('female', 1),\n",
              "  ('forge', 1),\n",
              "  ('founder', 1),\n",
              "  ('gain', 1),\n",
              "  ('gray', 1),\n",
              "  ('ground', 1),\n",
              "  ('high', 1),\n",
              "  ('kay', 2),\n",
              "  ('koplovitz', 2),\n",
              "  ('lead', 1),\n",
              "  ('leader', 1),\n",
              "  ('network', 1),\n",
              "  ('profile', 1),\n",
              "  ('say', 1),\n",
              "  ('science', 1),\n",
              "  ('successful', 1),\n",
              "  ('technology', 1),\n",
              "  ('usa', 1),\n",
              "  ('woman', 1)]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRX6eHXh-3D1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build LDA model\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=10, \n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha='auto',\n",
        "                                           per_word_topics=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfI23JNZ-90A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "7a89aad8-344f-48da-cc82-0e381ada073d"
      },
      "source": [
        "# Print the Keyword in the 10 topics\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0,\n",
            "  '0.076*\"business\" + 0.044*\"people\" + 0.037*\"idea\" + 0.028*\"opportunity\" + '\n",
            "  '0.025*\"world\" + 0.020*\"scholarship\" + 0.020*\"award\" + 0.016*\"panel\" + '\n",
            "  '0.015*\"benefit\" + 0.014*\"help\"'),\n",
            " (1,\n",
            "  '0.034*\"datum\" + 0.020*\"event\" + 0.020*\"value\" + 0.018*\"competition\" + '\n",
            "  '0.016*\"role\" + 0.016*\"get\" + 0.015*\"service\" + 0.013*\"development\" + '\n",
            "  '0.013*\"use\" + 0.012*\"pay\"'),\n",
            " (2,\n",
            "  '0.042*\"organisation\" + 0.037*\"research\" + 0.036*\"support\" + 0.031*\"provide\" '\n",
            "  '+ 0.026*\"develop\" + 0.024*\"impact\" + 0.016*\"grow\" + 0.015*\"polson\" + '\n",
            "  '0.013*\"similar\" + 0.013*\"agreement\"'),\n",
            " (3,\n",
            "  '0.033*\"audit\" + 0.024*\"encourage\" + 0.021*\"report\" + 0.018*\"study\" + '\n",
            "  '0.015*\"sustainable\" + 0.014*\"applicant\" + 0.014*\"working_world\" + '\n",
            "  '0.013*\"cognitive\" + 0.013*\"application\" + 0.013*\"building\"'),\n",
            " (4,\n",
            "  '0.022*\"recycling\" + 0.021*\"electrical\" + 0.020*\"waste\" + 0.019*\"equipment\" '\n",
            "  '+ 0.015*\"pitch\" + 0.015*\"return\" + 0.013*\"national\" + 0.012*\"deliver\" + '\n",
            "  '0.011*\"repair\" + 0.011*\"sydney\"'),\n",
            " (5,\n",
            "  '0.035*\"client\" + 0.031*\"experience\" + 0.024*\"purpose\" + 0.023*\"problem\" + '\n",
            "  '0.021*\"issue\" + 0.019*\"tax\" + 0.016*\"quality\" + 0.015*\"change\" + '\n",
            "  '0.015*\"sponsor\" + 0.014*\"run\"'),\n",
            " (6,\n",
            "  '0.035*\"skill\" + 0.031*\"model\" + 0.022*\"go\" + 0.021*\"part\" + 0.018*\"make\" + '\n",
            "  '0.017*\"investment\" + 0.016*\"receive\" + 0.016*\"international\" + '\n",
            "  '0.015*\"money\" + 0.014*\"customer\"'),\n",
            " (7,\n",
            "  '0.111*\"ey\" + 0.029*\"service\" + 0.028*\"firm\" + 0.024*\"big\" + 0.021*\"team\" + '\n",
            "  '0.021*\"global\" + 0.021*\"financial\" + 0.017*\"analytic\" + '\n",
            "  '0.016*\"professional\" + 0.015*\"project\"'),\n",
            " (8,\n",
            "  '0.060*\"student\" + 0.047*\"aaditya\" + 0.038*\"university\" + 0.031*\"work\" + '\n",
            "  '0.030*\"company\" + 0.024*\"win\" + 0.023*\"create\" + 0.019*\"build\" + '\n",
            "  '0.018*\"future\" + 0.017*\"organization\"'),\n",
            " (9,\n",
            "  '0.072*\"program\" + 0.040*\"engineering\" + 0.035*\"bachelor\" + 0.034*\"time\" + '\n",
            "  '0.024*\"community\" + 0.017*\"number\" + 0.017*\"focus\" + 0.017*\"large\" + '\n",
            "  '0.017*\"volunteer\" + 0.015*\"strategy\"')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ihk4OzdKApvf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "f3d22deb-b3c7-40d7-e94a-d73400cd5423"
      },
      "source": [
        "# Compute Perplexity\n",
        "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
        "\n",
        "# Compute Coherence Score\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Perplexity:  -8.104756899369283\n",
            "\n",
            "Coherence Score:  0.3953866657776465\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKe4dlKkBYsy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
        "vis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyZMtJ4N8Nnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "\n",
        "data_words = list(sent_to_words(data))\n",
        "\n",
        "print(data_words[:1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-89uK_2kWKLp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "exclude = set(string.punctuation) \n",
        "print(exclude)\n",
        "\n",
        "nopunctuation_word=[]\n",
        "for w in tokenized_word:\n",
        "    if w not in exclude:\n",
        "        nopunctuation_word.append(w)\n",
        "        \n",
        "print(\"nopunctuation:\",nopunctuation_word[:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29Yr-ffwWKLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.probability import FreqDist\n",
        "\n",
        "fdist = FreqDist(nopunctuation_word)\n",
        "print(fdist)\n",
        "fdist.most_common(20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7NwxdCfWKLt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fdist.plot(30,cumulative=False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0ULf45kWKLx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lower_word = [word.lower() for word in nopunctuation_word]\n",
        "print(lower_word[:100])\n",
        "\n",
        "filtered_word=[]\n",
        "for w in lower_word:\n",
        "    if w not in stop_words:\n",
        "        filtered_word.append(w)\n",
        "#print(\"nopunctuation:\",nopunctuation_word)\n",
        "print(\"Filtered:\",filtered_word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMa79iy5WKL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "stemmed_word=[]\n",
        "for w in filtered_word:\n",
        "    stemmed_word.append(ps.stem(w))\n",
        "\n",
        "#print(\"filtered:\",filtered_word)\n",
        "print(\"Stemmed:\",stemmed_word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IopnC8CCWKL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lem = WordNetLemmatizer()\n",
        "\n",
        "lm_word=[]\n",
        "for w in stemmed_word:\n",
        "    lm_word.append(lem.lemmatize(w,\"v\"))\n",
        "\n",
        "print(\"Lemmatized:\",lm_word[:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "923Sah4oWKL7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_word = [word.lower() for word in lm_word]\n",
        "print(clean_word[:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2icg2jiDWKL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fdist = FreqDist(clean_word)\n",
        "print(fdist)\n",
        "fdist.most_common(20)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "fdist.plot(30,cumulative=False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTtA2FR2WKL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import webtext\n",
        "from nltk.probability import FreqDist\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "nltk.download('webtext')\n",
        "#wt_words = webtext.words('testing.txt')  # Sample data\n",
        "data_analysis = nltk.FreqDist(clean_word)\n",
        " \n",
        "filter_words = dict([(m, n) for m, n in data_analysis.items() if len(m) > 1])\n",
        " \n",
        "wcloud = WordCloud().generate_from_frequencies(filter_words)\n",
        " \n",
        "# Plotting the wordcloud\n",
        "plt.imshow(wcloud, interpolation=\"bilinear\")\n",
        " \n",
        "plt.axis(\"on\")\n",
        "(-0.5, 499.5, 299.5, -0.5)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSESz5MJzF3_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dictionary = corpora.Dictionary(clean_word.tolist())\n",
        "count = 0\n",
        "for k, v in dictionary.iteritems():\n",
        "    print(k, v)\n",
        "    count += 1\n",
        "    if count > 10:\n",
        "        break\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bcuz_FoWKMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build the bigram and trigram models\n",
        "bigram = gensim.models.Phrases(clean_word, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[clean_word], threshold=100)  \n",
        "\n",
        "# Faster way to get a sentence clubbed as a trigram/bigram\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "\n",
        "# See trigram example\n",
        "print(bigram_mod[clean_word[3]])\n",
        "print(trigram_mod[bigram_mod[clean_word[3]]])\n",
        "print(bigram)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}