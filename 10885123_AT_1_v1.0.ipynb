{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject Number and Name: 94691 Deep Learning - Autumn 2019\n",
    "### Assessment Number                    : 1\n",
    "### Due Date                                        : April 22, 2019\n",
    "### Student Number                            : 10885123\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image # To grab the images and extract useful information\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "np.random.seed(42) # Set random seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4870, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notes: Added train_selected directory in my working directory.\n",
    "#        Extracted all files from train_selected.zip\n",
    "# Set the dataset directory\n",
    "dataset_dir = os.getcwd() + \"/train_selected\"\n",
    "\n",
    "# Get the data labels\n",
    "labels_file = dataset_dir + \"/train_selected.csv\"\n",
    "data_labels = pd.read_csv(labels_file)\n",
    "\n",
    "data_labels.shape\n",
    "# dataset_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get X files\n",
    "file_list = [dataset_dir + \"/\" + str(x) + \".png\" for x in list(data_labels[\"id\"])]\n",
    "# file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4370\n",
       "1     500\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the labels\n",
    "# finds column label with value = 'automobile' and assigns value 1 if true or 0 if false\n",
    "data_labels[\"class\"] = np.where(data_labels['label']=='automobile', 1, 0) \n",
    "# counts the number of unique values, sum of 1s and 0s\n",
    "data_labels[\"class\"].value_counts()\n",
    "# data_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Create a function that will standardise the dataset\n",
    "# Replace False\n",
    "\n",
    "def standarise_data(dataset):\n",
    "    # to standardise pixel data divide by 255\n",
    "    new_dataset = dataset/255.\n",
    "     \n",
    "    return new_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    global X_train, X_test, y_train, y_test, X, y\n",
    "\n",
    "    # loads the image files into an array, considers the number of pixels and colors.\n",
    "    X = np.array([np.array(Image.open(fname)) for fname in file_list])\n",
    "    \n",
    "    # loads the class into an array\n",
    "    y = np.array(data_labels[\"class\"])\n",
    "    \n",
    "    # sets the split of training ang testing datasets, random_state ensure that every run will provide the same results\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # y_train.shape[0] gets the number of rows\n",
    "    y_train = y_train.reshape(1, y_train.shape[0])\n",
    "    y_test = y_test.reshape(1, y_test.shape[0])\n",
    "    \n",
    "    # Reshape the training and test examples \n",
    "    X_train_f = X_train.reshape(X_train.shape[0], -1).T\n",
    "    X_test_f = X_test.reshape(X_test.shape[0], -1).T\n",
    "    \n",
    "       \n",
    "    # Standardize data to have feature values between 0 and 1.\n",
    "    X_train = standarise_data(X_train_f)\n",
    "    X_test = standarise_data(X_test_f)\n",
    "    \n",
    "    X_train = np.squeeze(X_train)\n",
    "    X_test = np.squeeze(X_test)\n",
    "    \n",
    "    print (\"Flatten X_train: \" + str(X_train.shape))\n",
    "    print (\"Flatten X_test: \" + str(X_test.shape))\n",
    "    \n",
    "    print (\"y_train: \" + str(y_train.shape))\n",
    "    print (\"y_test: \" + str(y_test.shape))\n",
    "    \n",
    "    # print (\"Flatten X_train_f: \" + str(X_train_f.shape))\n",
    "    # print (\"Flatten X_test_f: \" + str(X_test_f.shape))\n",
    "    \n",
    "       \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flatten X_train: (3072L, 3409L)\n",
      "Flatten X_test: (3072L, 1461L)\n",
      "y_train: (1L, 3409L)\n",
      "y_test: (1L, 1461L)\n"
     ]
    }
   ],
   "source": [
    "load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick 'normal' ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will find the following resources useful:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((3409L, 3072L), (1461L, 3072L), (3409L,), (1461L,))\n"
     ]
    }
   ],
   "source": [
    "X_train_clf = X_train.T\n",
    "X_test_clf = X_test.T\n",
    "\n",
    "y_train_clf = y_train.T.ravel()\n",
    "y_test_clf = y_test.T.ravel()\n",
    "\n",
    "print(X_train_clf.shape, X_test_clf.shape, y_train_clf.shape, y_test_clf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done   3 out of  10 | elapsed:  7.4min remaining: 17.2min\n",
      "[Parallel(n_jobs=7)]: Done  10 out of  10 | elapsed: 11.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]Total time taken: 0:11:15.212000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV \n",
    "import datetime\n",
    "\n",
    "C_list = np.linspace(0.001, 0.5, 20)\n",
    "log_reg = LogisticRegressionCV(\n",
    "    Cs=C_list, cv=10, penalty='l2', scoring='roc_auc', solver='liblinear', tol =1e-4, max_iter=1000, \n",
    "    class_weight='balanced', n_jobs=7, verbose=2, refit=True, multi_class='ovr', random_state=42\n",
    ")\n",
    "\n",
    "#Fit to our model\n",
    "start = datetime.datetime.now()\n",
    "log_reg.fit(X_train_clf, y_train_clf)\n",
    "end = datetime.datetime.now()\n",
    "print(\"Total time taken: {}\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the class\n",
    "y_test_clf = pd.DataFrame(y_test_clf, columns=[\"actual\"])\n",
    "y_test_clf[\"predictions_lr\"] = log_reg.predict(X_test_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Confustion Matrix \\n', array([[1154,  143],\n",
      "       [  53,  111]], dtype=int64))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.89      0.92      1297\n",
      "           1       0.44      0.68      0.53       164\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      1461\n",
      "   macro avg       0.70      0.78      0.73      1461\n",
      "weighted avg       0.90      0.87      0.88      1461\n",
      "\n",
      "('ROC-AUC Score \\n', 0.7832874174925251)\n",
      "('Accuracy Score \\n', 0.865845311430527)\n"
     ]
    }
   ],
   "source": [
    "# Get confusion matrix \n",
    "print(\"Confustion Matrix \\n\", confusion_matrix(y_test_clf.actual, y_test_clf.predictions_lr))\n",
    "\n",
    "# Get classification report\n",
    "print(classification_report(y_test_clf.actual, y_test_clf.predictions_lr))\n",
    "\n",
    "# Get ROC-AUC\n",
    "print(\"ROC-AUC Score \\n\", roc_auc_score(y_test_clf.actual, y_test_clf.predictions_lr))\n",
    "\n",
    "# Get accuracy\n",
    "print(\"Accuracy Score \\n\", accuracy_score(y_test_clf.actual, y_test_clf.predictions_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=7)]: Done 186 tasks      | elapsed:    9.4s\n",
      "[Parallel(n_jobs=7)]: Done 436 tasks      | elapsed:   21.2s\n",
      "[Parallel(n_jobs=7)]: Done 786 tasks      | elapsed:   37.7s\n",
      "[Parallel(n_jobs=7)]: Done 1000 out of 1000 | elapsed:   47.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 0:01:03.178000\n"
     ]
    }
   ],
   "source": [
    "# A tree based example\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import datetime\n",
    "\n",
    "#Create the model object\n",
    "rf_class = RandomForestClassifier(\n",
    "    n_estimators=1000, criterion='entropy', \n",
    "    max_depth=15, min_samples_split=3, bootstrap=True, oob_score=True, \n",
    "    n_jobs=7, random_state=42, verbose=1, class_weight='balanced' \n",
    ")\n",
    "\n",
    "#Fit to our model\n",
    "start = datetime.datetime.now()\n",
    "rf_class.fit(X_train_clf, y_train_clf)\n",
    "end = datetime.datetime.now()\n",
    "print(\"Total time taken: {}\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Using backend ThreadingBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=7)]: Done 186 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=7)]: Done 436 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=7)]: Done 786 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=7)]: Done 1000 out of 1000 | elapsed:    0.8s finished\n"
     ]
    }
   ],
   "source": [
    "#Predict the class\n",
    "y_test_clf[\"predictions_rf\"] = rf_class.predict(X_test_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Confustion Matrix \\n', array([[1297,    0],\n",
      "       [ 151,   13]], dtype=int64))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.94      1297\n",
      "           1       1.00      0.08      0.15       164\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      1461\n",
      "   macro avg       0.95      0.54      0.55      1461\n",
      "weighted avg       0.91      0.90      0.86      1461\n",
      "\n",
      "('ROC-AUC Score \\n', 0.5396341463414634)\n",
      "('Accuracy Score \\n', 0.8966461327857632)\n"
     ]
    }
   ],
   "source": [
    "# Get confusion matrix \n",
    "print(\"Confustion Matrix \\n\", confusion_matrix(y_test_clf.actual, y_test_clf.predictions_rf))\n",
    "\n",
    "# Get classification report\n",
    "print(classification_report(y_test_clf.actual, y_test_clf.predictions_rf))\n",
    "\n",
    "# Get ROC-AUC\n",
    "print(\"ROC-AUC Score \\n\", roc_auc_score(y_test_clf.actual, y_test_clf.predictions_rf))\n",
    "\n",
    "# Get accuracy\n",
    "print(\"Accuracy Score \\n\", accuracy_score(y_test_clf.actual, y_test_clf.predictions_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "The model will have the following layers\n",
    "     - 1 input layer (will be the number of rows of input (i.e. number of training/test records)\n",
    "     - 3 hidden layers (hidden layer sizes 10,25,10)\n",
    "     - Relu activation function for all layers except the last layer\n",
    "     - last layer will be a sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Correctly create the layer dimensions as per the brief\n",
    "# Replace False\n",
    "# For example layer_dimensions of [5,7,2,1] would be input 5, two hidden layers (7,2) and 1 in output\n",
    "\n",
    "layer_dimensions = ([X_train.shape[0],10,25,10,1])\n",
    "#len(layer_dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Weights and biases are initialised using the He initialisation. \n",
    "- He initialisation uses a scaling factor of sqrt(2./layers_dimensions[l-1]) for the weights.\n",
    "- Each layer will have a calculated weights and biases and will be stored in a parameters dictionary.\n",
    "\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector: numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "Observations:\n",
    "- I initially used 0.01 as the scaling factor for the weights and biases. However, i have noticed that the model is not able to make accurate predictions.\n",
    "- The probabilities are the same resulting to a horizontal line when plotted.\n",
    "- there will be 5 layers in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Use your knowledge of parameter matrix size to edit the code. \n",
    "# Replace False\n",
    "\n",
    "def initialise_parameters(layer_dimensions):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    layer_dimensions -- python (list), one item per layer, number representing size of layer\n",
    "    \n",
    "    Output:\n",
    "    parameters -- python dictionary containing your weight and bias parameters \"W1\", \"b1\", ..., \"WL\", \"bL\" \n",
    "                  with appropriate sizes.\n",
    "    \"\"\"\n",
    "    \n",
    "    global parameters\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    parameters = {}\n",
    "    L = len(layer_dimensions)  \n",
    "    \n",
    "    for l in range(1, L):\n",
    "#       parameters['W' + str(l)] = np.random.randn(layer_dimensions[l], layer_dimensions[l-1]) * 0.01 \n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dimensions[l], layer_dimensions[l-1]) * np.sqrt(2./layer_dimensions[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dimensions[l], 1)) \n",
    "        # print(l)\n",
    "    \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a series of functions that will:\n",
    "- Undertake the linear multiplication\n",
    "- Underake the activation of the layer\n",
    "- Store this somewhere for efficient computation of backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activations\n",
    "We will need activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Sigmoid function (σ): g(z) = 1 / (1 + e^{-z}). It is recommended to be used only on the output layer so that we can easily interpret the output as probabilities since it has restricted output between 0 and 1. \n",
    "\n",
    "- Rectified Linear Unit (ReLU): g(z) = max{0, z}. It shares a  lot of similar properties with linear function. Used for hidden layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Create a function that will undertake sigmoid activaiton\n",
    "# Create another function that will undertake relu activation\n",
    "# Replace False\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"    \n",
    "    Input:\n",
    "    Z     -- numpy array of any shape\n",
    "    \n",
    "    Output:\n",
    "    A     -- output of sigmoid(z), (should be same shape as Z!)\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "        \n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"    \n",
    "    Input:\n",
    "    Z     -- numpy array of any shape\n",
    "    \n",
    "    Output:\n",
    "    A     -- output of relu(z), (should be same shape as Z!)\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0, Z)\n",
    "    \n",
    "    cache = Z \n",
    "    \n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Prop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Given the input from previous layer, the linear function is calculated by Z[l]=W[l]A[l−1]+b[l].\n",
    "\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: \n",
    "    b -- bias vector\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function. \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Create a function that will undertake the linear component of forward prop\n",
    "# Replace False\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    A     -- activations from previous layer\n",
    "    W     -- weights matrix\n",
    "    b     -- bias vector\n",
    "\n",
    "    Output:\n",
    "    Z     -- the input to activation function \n",
    "    cache -- a python dictionary with \"A\", \"W\" and \"b\" for backprop\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W,A) + b\n",
    "    \n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Implements the linear and activation function based on the input function (\"sigmoid\" or \"relu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# This function conditionally calls an activation function. \n",
    "# Call the correction function above with the correct if statement\n",
    "# Replace False\n",
    "\n",
    "def activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "\n",
    "    Input:\n",
    "    A_prev     -- activations from previous layer\n",
    "    W          -- weights matrix\n",
    "    b          -- bias vector\n",
    "    activation -- the activation type to be used (\"sigmoid\" or \"relu\")\n",
    "\n",
    "    Output:\n",
    "    A          -- the output of the activation function, also called the post-activation value \n",
    "    cache      -- a python dictionary with two two caches \"linear_cache\" and \"activation_cache\" for backprop\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    ###NOTE###\n",
    "    # This is where you can put more activation functions for the extension tasks\n",
    "    \n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- implements the total_forward in the following order;    \n",
    "\n",
    "1. LINEAR\n",
    "   - implement linear using the input\n",
    "2. LINEAR -> ACTIVATION where ACTIVATION will be either ReLU or Sigmoid.\n",
    "   - implements the activation_forward function based on l number of hidden layers (if function is \"relu\")\n",
    "   - implements the activation_forward function based on the L last layer (if function is \"sigmoid\")\n",
    "3. [LINEAR -> RELU] × (L-1) -> LINEAR -> SIGMOID (whole model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Architect the forward pass. \n",
    "# You will need to firstly determine how many layers there are\n",
    "# You will then need to pull out the correct parameters we initalised\n",
    "# Ensure you use the appropriate activation for the middle layers\n",
    "# Pay special attention to the last layer\n",
    "# It may help to print out parameters\n",
    "# Replace False\n",
    "\n",
    "def total_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    \n",
    "    Input:\n",
    "    X            -- raw data\n",
    "    parameters   -- dictionary of initialised parameters, output from a particular function above.\n",
    "    \n",
    "    Returns:\n",
    "    AL           -- last post-activation value\n",
    "    caches       -- list of caches from forward activations\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(layer_dimensions) - 1\n",
    "        \n",
    "    \n",
    "    # All the layers up until the last (sigmoid) layer\n",
    "    for l in range(1, L):\n",
    "  \n",
    "        A_prev = A \n",
    "        A, cache = activation_forward(A_prev, \n",
    "                                      parameters['W' + str(l)], \n",
    "                                      parameters['b' + str(l)], \n",
    "                                      activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "                                                 \n",
    "    # The last layer - how do we use the sigmoid function?\n",
    "    \n",
    "    AL, cache = activation_forward(A, \n",
    "                                      parameters['W' + str(L)], \n",
    "                                      parameters['b' + str(L)], \n",
    "                                      activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "            \n",
    "    return AL, caches\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backwards Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Back propagation is used to calculate the gradients(slope/derivatives) of the loss function with respect to the model parameters(w,b). Compute for gradient to update the parameters (W,b) to minimise cost for each iteration.\n",
    "\n",
    "- Derivative formula used;\n",
    "  - dZ = np.where(Z < 0, 0, dZ)   - derivative of relu \n",
    "  - dZ = dA * s * (1-s) - derivative of sigmoid where s = 1 / (1 + np.exp(-Z)) (sigmoid function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Differentiate the relu and the sigmoid functions\n",
    "# Replace False\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    dA      -- post-activation gradient\n",
    "    cache   -- 'Z' that is used in the backwards prop here.\n",
    "\n",
    "    Output:\n",
    "    dZ      -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.copy(dA) # Copying dA first\n",
    "    \n",
    "    # What do you set dZ to when Z is what values? \n",
    "    # dZ = np.multiply(dA, np.int64(Z > 0))\n",
    "    dZ = np.where(Z < 0, 0, dZ)\n",
    "        \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input:\n",
    "    dA      -- post-activation gradient\n",
    "    cache   -- 'Z' that is used in backprop here\n",
    "\n",
    "    Returns:\n",
    "    dZ      -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    s = 1 / (1 + np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Calculate the back propagation gradient with respect to W and b.\n",
    "\n",
    "    dW = 1./m * np.dot(dZ,A_prev.T)  -- Gradient of the cost w.r.t W of current layer\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True) - Gradient of the cost w.r.t b of current layer l\n",
    "    dA_prev = np.dot(W.T,dZ) - Gradient of the cost w.r.t activation of previous layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# You do not need to do anything here, but notes are included for your interest\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    dZ        -- Gradient of the cost with respect to 'Z' of current layer\n",
    "    cache     -- (A_prev, W, b) from forward propag in the current layer, we stored this previously\n",
    "\n",
    "    Output:\n",
    "    dA_prev   -- Gradient of the cost w.r.t activation of previous layer\n",
    "    dW        -- Gradient of the cost w.r.t W of current layer\n",
    "    db        -- Gradient of the cost w.r.t b of current layer l\n",
    "    \"\"\"\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "Implements the linear_backward and activation backward functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Use the activation differentiation functions you created above\n",
    "# Ensure you are putting the right arguments (hint: caches) into the functions\n",
    "# For the first false, consider what function give back dZ? (what does it require?)\n",
    "# Replace False\n",
    "\n",
    "def activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    dA         -- post-activation gradient for current layer\n",
    "    cache      -- (linear_cache, activation_cache) stored previously for backprop\n",
    "    activation -- activation for this layer (\"sigmoid\" or \"relu\")\n",
    "    \n",
    "    Output:\n",
    "    dA_prev   -- Gradient of the cost w.r.t activation of previous layer\n",
    "    dW        -- Gradient of the cost w.r.t W of current layer\n",
    "    db        -- Gradient of the cost w.r.t b of current layer l\n",
    "    \"\"\"\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache) \n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache) \n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Implements the total_backward function in three steps;\n",
    "\n",
    "1. LINEAR backward (Initializing the backpropagation, derivative of the cost with respect to output (AL))\n",
    "   - dAL = - (np.divide(Y,AL) - np.divide(1 - Y, 1 - AL))\n",
    "2. LINEAR -> ACTIVATION backward where ACTIVATION computes the derivative of either the ReLU or sigmoid activation\n",
    "3. [LINEAR -> RELU] ×× (L-1) -> LINEAR -> SIGMOID backward (whole model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Differentiate the loss function with respect to the last activation layer\n",
    "# Replace False\n",
    "\n",
    "def total_backward(AL, Y, caches):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input:\n",
    "    AL        -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y         -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches    -- list of caches from relu and sigmoid we kept from forward prop\n",
    "    \n",
    "    output:\n",
    "    grads     -- A dictionary with the gradients named dA+l,dW+l, db+l for each layer\n",
    "    \"\"\"\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y,AL) - np.divide(1 - Y, 1 - AL))\n",
    "    # dAL = np.divide(AL - y, np.multiply(AL, 1 - AL))\n",
    "    # dAL = AL * (1-AL)\n",
    "\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Compute the cross-entropy cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Write a function to compute the binary logistic cost function ('cross entropy loss')\n",
    "# This is on page 51 of the slides from block_1. \n",
    "# You may need to transpose elements to make the matrix calculations work\n",
    "# Replace False\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    AL    -- probability vector for label predictions\n",
    "    Y     -- truth vector vector\n",
    "\n",
    "    Output:\n",
    "    cost  -- cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    # Compute loss from aL and y.\n",
    "    cost_total =  - (np.dot(Y,np.log(AL).T) + np.dot(1- Y,np.log(1-AL).T))\n",
    "    cost = (1./m) * cost_total \n",
    "    \n",
    "    cost = np.squeeze(cost) # Help with the shape\n",
    "    # assert(cost.shape == ())\n",
    "    \n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "Once we have computed the gradients, multiply them with a factor called learning-rate (converging rate) and subtract from the initial parameters to get the updated parameters(weights and biases). Learning rate should be minimal so that we will not miss the global minimum point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Update each parameter\n",
    "# Remember what hyperparameter is important for this step?\n",
    "# You will also find a useful, indexed value in the 'grads' dictionary created in backprop above\n",
    "# Replace False\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    \n",
    "    Input:\n",
    "    parameters    -- dictionary with parameters \n",
    "    grads         -- dictionary with gradients (which function outputs this?)\n",
    "    learning_date -- step size to adjust parameters by\n",
    "    \n",
    "    Returns:\n",
    "    parameters    -- dictionary containing your updated parameters , same structure as original parameters dict\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 ### number of layers\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "Create a function to knit together everything you have done so far and allow for different layer sizes and lengths to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Now stitch it all together. Essentially you will need to call all your functions in turn with the right arguments.\n",
    "# Initialise parameters\n",
    "# Undertake forward prop. What is our master function? Consider what we got from initialisation?\n",
    "# Undertake backwards prop. Again consider our master function for back prop.\n",
    "# Update parameters.\n",
    "# Replace False\n",
    "\n",
    "def total_backward_forward(X, Y, layers_dimensions, \n",
    "                           learning_rate, \n",
    "                           num_iterations, \n",
    "                           print_cost):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input:\n",
    "    X                 -- data\n",
    "    Y                 -- truth vector (1,0)'s\n",
    "    layers_dimensions -- list of dimensions for each layer of network\n",
    "    learning_rate     -- step size for gradient descent\n",
    "    num_iterations    -- number of training iterations to undertake\n",
    "    print_cost        -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    output:\n",
    "    parameters        -- parameters learnt by the model. Used to predict\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(42)\n",
    "    costs = []\n",
    "    \n",
    "    # Parameters initialization\n",
    "    parameters = initialise_parameters(layer_dimensions)\n",
    "     \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation:\n",
    "        AL, caches = total_forward(X, parameters)\n",
    "         \n",
    "        # Compute cost\n",
    "        cost = compute_cost(AL, Y)\n",
    "            \n",
    "        # Backward propagation.\n",
    "        grads = total_backward(AL, Y, caches)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.405542\n",
      "Cost after iteration 100: 0.294678\n",
      "Cost after iteration 200: 0.254238\n",
      "Cost after iteration 300: 0.228810\n",
      "Cost after iteration 400: 0.214546\n",
      "Cost after iteration 500: 0.204157\n",
      "Cost after iteration 600: 0.202840\n",
      "Cost after iteration 700: 0.196585\n",
      "Cost after iteration 800: 0.190883\n",
      "Cost after iteration 900: 0.186656\n",
      "Cost after iteration 1000: 0.182032\n",
      "Cost after iteration 1100: 0.178138\n",
      "Cost after iteration 1200: 0.174165\n",
      "Cost after iteration 1300: 0.170951\n",
      "Cost after iteration 1400: 0.167312\n",
      "Cost after iteration 1500: 0.165311\n",
      "Cost after iteration 1600: 0.161649\n",
      "Cost after iteration 1700: 0.158446\n",
      "Cost after iteration 1800: 0.155142\n",
      "Cost after iteration 1900: 0.152336\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = total_backward_forward(X_train, \n",
    "                                    y_train, \n",
    "                                    layer_dimensions, \n",
    "                                    learning_rate = 0.01,\n",
    "                                    num_iterations = 2000, \n",
    "                                    print_cost = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict (Hold out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Create your own predict function.\n",
    "# Note the number of training examples\n",
    "# Turn the probabilities into 0-1 predictions\n",
    "# Replace False\n",
    "\n",
    "def predict(X, y, parameters):\n",
    "    \"\"\" \n",
    "    Input:\n",
    "    X           -- data (test set)\n",
    "    parameters  -- parameters of the trained model\n",
    "    \n",
    "    Output:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0] # How many training examples?\n",
    "    print('m: ' + str(m))\n",
    "    n = len(parameters) // 2\n",
    "    print('n: ' + str(n))\n",
    "    p = np.zeros((1,m)) # Initialise probabilities to zero\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = total_forward(X, parameters)\n",
    "    print('probas: ' + str(probas))\n",
    "    \n",
    "    # convert probas to 0/1 predictions. \n",
    "    p = np.where(probas >= 0.5,1,0)\n",
    "            \n",
    "    return p, probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m: 3072\n",
      "n: 4\n",
      "probas: [[0.27615632 0.07601046 0.00626281 ... 0.00165396 0.17539389 0.08161842]]\n"
     ]
    }
   ],
   "source": [
    "# Create some predictions\n",
    "predictions, probas = predict(X_test, y_test, parameters)\n",
    "# print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2b1295c0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make a scatter plot of probabilities. Good check if something is wrong\n",
    "plt.scatter(range(len(probas[0])), probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1327\n",
       "1     134\n",
       "Name: prediction, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check your prediction value counts\n",
    "pred_df = pd.DataFrame(predictions[0], columns=[\"prediction\"])\n",
    "pred_df.prediction.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1461L, 1L)\n",
      "(1461L, 1L)\n"
     ]
    }
   ],
   "source": [
    "# Do a bit of reshaping\n",
    "predictions_sk = predictions.reshape(len(predictions[0]), 1)\n",
    "print(predictions_sk.shape)\n",
    "\n",
    "y_test_sk = y_test.T\n",
    "print(y_test_sk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Confustion Matrix \\n', array([[1258,   39],\n",
      "       [  69,   95]], dtype=int64))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96      1297\n",
      "           1       0.71      0.58      0.64       164\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1461\n",
      "   macro avg       0.83      0.77      0.80      1461\n",
      "weighted avg       0.92      0.93      0.92      1461\n",
      "\n",
      "('Accuracy: ', 0.9260780287474333)\n",
      "('ROC_AUC: ', 0.870728886548696)\n"
     ]
    }
   ],
   "source": [
    "# Build some sklearn scores\n",
    "\n",
    "#Get confusion matrix \n",
    "print(\"Confustion Matrix \\n\", confusion_matrix(list(y_test_sk), list(predictions_sk)))\n",
    "\n",
    "#Get classification report\n",
    "print(classification_report(y_test_sk, predictions_sk))\n",
    "\n",
    "# Accuracy score\n",
    "print(\"Accuracy: \", accuracy_score(y_test_sk, predictions_sk))\n",
    "\n",
    "# ROC_AUC score\n",
    "print(\"ROC_AUC: \", roc_auc_score(y_test_sk, probas.T))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
