{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image # To grab the images and extract useful information\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "np.random.seed(42) # Set random seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4870, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notes: Added train_selected directory in my working directory.\n",
    "#        Extracted all files from train_selected.zip\n",
    "# Set the dataset directory\n",
    "dataset_dir = os.getcwd() + \"/train_selected\"\n",
    "\n",
    "# Get the data labels\n",
    "labels_file = dataset_dir + \"/train_selected.csv\"\n",
    "data_labels = pd.read_csv(labels_file)\n",
    "\n",
    "data_labels.shape\n",
    "# dataset_dir\n",
    "# Output: (4870, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get X files\n",
    "file_list = [dataset_dir + \"/\" + str(x) + \".png\" for x in list(data_labels[\"id\"])]\n",
    "# file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4370\n",
       "1     500\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the labels\n",
    "# finds column label with value = 'automobile' and assigns value 1 if true or 0 if false\n",
    "data_labels[\"class\"] = np.where(data_labels['label']=='automobile', 1, 0) \n",
    "# counts the number of unique values, sum of 1s and 0s\n",
    "data_labels[\"class\"].value_counts()\n",
    "# data_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Create a function that will standardise the dataset\n",
    "# Replace False\n",
    "\n",
    "def standarise_data(dataset):\n",
    "    # to standardise pixel data divide by 255\n",
    "    new_dataset = dataset/255.\n",
    "     \n",
    "    return new_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    global X_train, X_test, y_train, y_test, X, y\n",
    "\n",
    "    # loads the image files into an array, considers the number of pixels and colors.\n",
    "    X = np.array([np.array(Image.open(fname)) for fname in file_list])\n",
    "    \n",
    "    # loads the class into an array\n",
    "    y = np.array(data_labels[\"class\"])\n",
    "    \n",
    "    # sets the split of training ang testing datasets, random_state ensure that every run will provide the same results\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # y_train.shape[0] gets the number of rows\n",
    "    y_train = y_train.reshape(1, y_train.shape[0])\n",
    "    y_test = y_test.reshape(1, y_test.shape[0])\n",
    "    \n",
    "    # Reshape the training and test examples \n",
    "    X_train_f = X_train.reshape(X_train.shape[0], -1).T\n",
    "    X_test_f = X_test.reshape(X_test.shape[0], -1).T\n",
    "    \n",
    "       \n",
    "    # Standardize data to have feature values between 0 and 1.\n",
    "    X_train = standarise_data(X_train_f)\n",
    "    X_test = standarise_data(X_test_f)\n",
    "    \n",
    "    X_train = np.squeeze(X_train)\n",
    "    X_test = np.squeeze(X_test)\n",
    "    \n",
    "    print (\"Flatten X_train: \" + str(X_train.shape))\n",
    "    print (\"Flatten X_test: \" + str(X_test.shape))\n",
    "    \n",
    "    print (\"y_train: \" + str(y_train.shape))\n",
    "    print (\"y_test: \" + str(y_test.shape))\n",
    "    \n",
    "    # print (\"Flatten X_train_f: \" + str(X_train_f.shape))\n",
    "    # print (\"Flatten X_test_f: \" + str(X_test_f.shape))\n",
    "    \n",
    "       \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flatten X_train: (3072L, 3409L)\n",
      "Flatten X_test: (3072L, 1461L)\n",
      "y_train: (1L, 3409L)\n",
      "y_test: (1L, 1461L)\n"
     ]
    }
   ],
   "source": [
    "load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick 'normal' ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will find the following resources useful:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "X_train_clf = X_train.T\n",
    "X_test_clf = X_test.T\n",
    "\n",
    "y_train_clf = y_train.T.ravel()\n",
    "y_test_clf = y_test.T.ravel()\n",
    "\n",
    "print(X_train_clf.shape, X_test_clf.shape, y_train_clf.shape, y_test_clf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV \n",
    "import datetime\n",
    "\n",
    "C_list = np.linspace(0.001, 0.5, 20)\n",
    "log_reg = LogisticRegressionCV(\n",
    "    Cs=C_list, cv=10, penalty='l2', scoring='roc_auc', solver='liblinear', tol =1e-4, max_iter=1000, \n",
    "    class_weight='balanced', n_jobs=7, verbose=2, refit=True, multi_class='ovr', random_state=42\n",
    ")\n",
    "\n",
    "#Fit to our model\n",
    "start = datetime.datetime.now()\n",
    "log_reg.fit(X_train_clf, y_train_clf)\n",
    "end = datetime.datetime.now()\n",
    "print(\"Total time taken: {}\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#Predict the class\n",
    "y_test_clf = pd.DataFrame(y_test_clf, columns=[\"actual\"])\n",
    "y_test_clf[\"predictions_lr\"] = log_reg.predict(X_test_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Get confusion matrix \n",
    "print(\"Confustion Matrix \\n\", confusion_matrix(y_test_clf.actual, y_test_clf.predictions_lr))\n",
    "\n",
    "# Get classification report\n",
    "print(classification_report(y_test_clf.actual, y_test_clf.predictions_lr))\n",
    "\n",
    "# Get ROC-AUC\n",
    "print(\"ROC-AUC Score \\n\", roc_auc_score(y_test_clf.actual, y_test_clf.predictions_lr))\n",
    "\n",
    "# Get accuracy\n",
    "print(\"Accuracy Score \\n\", accuracy_score(y_test_clf.actual, y_test_clf.predictions_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# A tree based example\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import datetime\n",
    "\n",
    "#Create the model object\n",
    "rf_class = RandomForestClassifier(\n",
    "    n_estimators=1000, criterion='entropy', \n",
    "    max_depth=15, min_samples_split=3, bootstrap=True, oob_score=True, \n",
    "    n_jobs=7, random_state=42, verbose=1, class_weight='balanced' \n",
    ")\n",
    "\n",
    "#Fit to our model\n",
    "start = datetime.datetime.now()\n",
    "rf_class.fit(X_train_clf, y_train_clf)\n",
    "end = datetime.datetime.now()\n",
    "print(\"Total time taken: {}\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#Predict the class\n",
    "y_test_clf[\"predictions_rf\"] = rf_class.predict(X_test_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Get confusion matrix \n",
    "print(\"Confustion Matrix \\n\", confusion_matrix(y_test_clf.actual, y_test_clf.predictions_rf))\n",
    "\n",
    "# Get classification report\n",
    "print(classification_report(y_test_clf.actual, y_test_clf.predictions_rf))\n",
    "\n",
    "# Get ROC-AUC\n",
    "print(\"ROC-AUC Score \\n\", roc_auc_score(y_test_clf.actual, y_test_clf.predictions_rf))\n",
    "\n",
    "# Get accuracy\n",
    "print(\"Accuracy Score \\n\", accuracy_score(y_test_clf.actual, y_test_clf.predictions_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Correctly create the layer dimensions as per the brief\n",
    "# Replace False\n",
    "\n",
    "# For example layer_dimensions of [5,7,2,1] would be input 5, two hidden layers (7,2) and 1 in output\n",
    "\n",
    "layer_dimensions = ([X_train.shape[0],10,25,10,1])\n",
    "#layer_dimensions = [3072,10,25,10,1]\n",
    "#layer_dimensions[0]\n",
    "#len(layer_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Use your knowledge of parameter matrix size to edit the code. \n",
    "# Replace False\n",
    "\n",
    "def initialise_parameters(layer_dimensions):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    layer_dimensions -- python (list), one item per layer, number representing size of layer\n",
    "    \n",
    "    Output:\n",
    "    parameters -- python dictionary containing your weight and bias parameters \"W1\", \"b1\", ..., \"WL\", \"bL\" \n",
    "                  with appropriate sizes.\n",
    "    \"\"\"\n",
    "    \n",
    "    global parameters\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    parameters = {}\n",
    "    L = len(layer_dimensions)  \n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dimensions[l], layer_dimensions[l-1]) * 0.01 \n",
    "        parameters['b' + str(l)] = np.zeros((layer_dimensions[l], 1))\n",
    "        # print(l)\n",
    "    \n",
    "    return parameters\n",
    "#initialise_parameters(layer_dimensions)\n",
    "#len(parameters)\n",
    "#range(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward prop\n",
    "Create a series of functions that will:\n",
    "\n",
    "Undertake the linear multiplication\n",
    "Underake the activation of the layer\n",
    "Store this somewhere for efficient computation of backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activations\n",
    "We will need activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Create a function that will undertake sigmoid activaiton\n",
    "# Create another function that will undertake relu activation\n",
    "# Replace False\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"    \n",
    "    Input:\n",
    "    Z     -- numpy array of any shape\n",
    "    \n",
    "    Output:\n",
    "    A     -- output of sigmoid(z), (should be same shape as Z!)\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "        \n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"    \n",
    "    Input:\n",
    "    Z     -- numpy array of any shape\n",
    "    \n",
    "    Output:\n",
    "    A     -- output of relu(z), (should be same shape as Z!)\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0, Z)\n",
    "    \n",
    "    cache = Z \n",
    "    \n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Create a function that will undertake the linear component of forward prop\n",
    "# Replace False\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    A     -- activations from previous layer\n",
    "    W     -- weights matrix\n",
    "    b     -- bias vector\n",
    "\n",
    "    Output:\n",
    "    Z     -- the input to activation function \n",
    "    cache -- a python dictionary with \"A\", \"W\" and \"b\" for backprop\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W,A) + b\n",
    "    \n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# This function conditionally calls an activation function. \n",
    "# Call the correction function above with the correct if statement\n",
    "# Replace False\n",
    "\n",
    "def activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "\n",
    "    Input:\n",
    "    A_prev     -- activations from previous layer\n",
    "    W          -- weights matrix\n",
    "    b          -- bias vector\n",
    "    activation -- the activation type to be used (\"sigmoid\" or \"relu\")\n",
    "\n",
    "    Output:\n",
    "    A          -- the output of the activation function, also called the post-activation value \n",
    "    cache      -- a python dictionary with two two caches \"linear_cache\" and \"activation_cache\" for backprop\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    ###NOTE###\n",
    "    # This is where you can put more activation functions for the extension tasks\n",
    "    \n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Architect the forward pass. \n",
    "# You will need to firstly determine how many layers there are\n",
    "# You will then need to pull out the correct parameters we initalised\n",
    "# Ensure you use the appropriate activation for the middle layers\n",
    "# Pay special attention to the last layer\n",
    "# It may help to print out parameters\n",
    "# Replace False\n",
    "\n",
    "def total_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    \n",
    "    Input:\n",
    "    X            -- raw data\n",
    "    parameters   -- dictionary of initialised parameters, output from a particular function above.\n",
    "    \n",
    "    Returns:\n",
    "    AL           -- last post-activation value\n",
    "    caches       -- list of caches from forward activations\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(layer_dimensions) - 1\n",
    "        \n",
    "    \n",
    "    # All the layers up until the last (sigmoid) layer\n",
    "    for l in range(1, L):\n",
    "        #print(l)\n",
    "        A_prev = A \n",
    "        A, cache = activation_forward(A_prev, \n",
    "                                      parameters['W' + str(l)], \n",
    "                                      parameters['b' + str(l)], \n",
    "                                      activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "                                                 \n",
    "    # The last layer - how do we use the sigmoid function?\n",
    "    \n",
    "    AL, cache = activation_forward(A, \n",
    "                                      parameters['W' + str(L)], \n",
    "                                      parameters['b' + str(L)], \n",
    "                                      activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "            \n",
    "    return AL, caches\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backwards Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Differentiate the relu and the sigmoid functions\n",
    "# Replace False\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    dA      -- post-activation gradient\n",
    "    cache   -- 'Z' that is used in the backwards prop here.\n",
    "\n",
    "    Output:\n",
    "    dZ      -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.copy(dA) # Copying dA first\n",
    "    \n",
    "    # What do you set dZ to when Z is what values? \n",
    "    # dZ = np.multiply(dA, np.int64(Z > 0))\n",
    "    dZ = np.where(Z < 0, 0, dZ)\n",
    "        \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input:\n",
    "    dA      -- post-activation gradient\n",
    "    cache   -- 'Z' that is used in backprop here\n",
    "\n",
    "    Returns:\n",
    "    dZ      -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    s = 1 / (1 + np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# You do not need to do anything here, but notes are included for your interest\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    dZ        -- Gradient of the cost with respect to 'Z' of current layer\n",
    "    cache     -- (A_prev, W, b) from forward propag in the current layer, we stored this previously\n",
    "\n",
    "    Output:\n",
    "    dA_prev   -- Gradient of the cost w.r.t activation of previous layer\n",
    "    dW        -- Gradient of the cost w.r.t W of current layer\n",
    "    db        -- Gradient of the cost w.r.t b of current layer l\n",
    "    \"\"\"\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1./m * (np.dot(dZ,A_prev.T))\n",
    "    db = 1./m * (np.sum(dZ, axis = 1, keepdims = True))\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    #dA_prev = np.dot(W,dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Use the activation differentiation functions you created above\n",
    "# Ensure you are putting the right arguments (hint: caches) into the functions\n",
    "# For the first false, consider what function give back dZ? (what does it require?)\n",
    "# Replace False\n",
    "\n",
    "def activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    dA         -- post-activation gradient for current layer\n",
    "    cache      -- (linear_cache, activation_cache) stored previously for backprop\n",
    "    activation -- activation for this layer (\"sigmoid\" or \"relu\")\n",
    "    \n",
    "    Output:\n",
    "    dA_prev   -- Gradient of the cost w.r.t activation of previous layer\n",
    "    dW        -- Gradient of the cost w.r.t W of current layer\n",
    "    db        -- Gradient of the cost w.r.t b of current layer l\n",
    "    \"\"\"\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache) \n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache) \n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Differentiate the loss function with respect to the last activation layer\n",
    "# Replace False\n",
    "\n",
    "def total_backward(AL, Y, caches):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Input:\n",
    "    AL        -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y         -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches    -- list of caches from relu and sigmoid we kept from forward prop\n",
    "    \n",
    "    output:\n",
    "    grads     -- A dictionary with the gradients named dA+l,dW+l, db+l for each layer\n",
    "    \"\"\"\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y,AL) - np.divide(1 - Y, 1 - AL))\n",
    "    # dAL = np.divide(AL - y, np.multiply(AL, 1 - AL))\n",
    "    # dAL = AL * (1-AL)\n",
    "\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        dA_prev_temp, dW_temp, db_temp = activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Write a function to compute the binary logistic cost function ('cross entropy loss')\n",
    "# This is on page 51 of the slides from block_1. \n",
    "# You may need to transpose elements to make the matrix calculations work\n",
    "# Replace False\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    AL    -- probability vector for label predictions\n",
    "    Y     -- truth vector vector\n",
    "\n",
    "    Output:\n",
    "    cost  -- cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    #print(AL.shape[1])\n",
    "    #print(m)\n",
    "    \n",
    "    # Compute loss from aL and y.\n",
    "    cost_total =  - (np.dot(Y,np.log(AL).T) + np.dot(1- Y,np.log(1-AL).T))\n",
    "    cost = (1./m) * cost_total \n",
    "    \n",
    "    cost = np.squeeze(cost) # Help with the shape\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Update each parameter\n",
    "# Remember what hyperparameter is important for this step?\n",
    "# You will also find a useful, indexed value in the 'grads' dictionary created in backprop above\n",
    "# Replace False\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    \n",
    "    Input:\n",
    "    parameters    -- dictionary with parameters \n",
    "    grads         -- dictionary with gradients (which function outputs this?)\n",
    "    learning_date -- step size to adjust parameters by\n",
    "    \n",
    "    Returns:\n",
    "    parameters    -- dictionary containing your updated parameters , same structure as original parameters dict\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 ### number of layers\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to knit together everything you have done so far and allow for different layer sizes and lengths to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Now stitch it all together. Essentially you will need to call all your functions in turn with the right arguments.\n",
    "# Initialise parameters\n",
    "# Undertake forward prop. What is our master function? Consider what we got from initialisation?\n",
    "# Undertake backwards prop. Again consider our master function for back prop.\n",
    "# Update parameters.\n",
    "# Replace False\n",
    "\n",
    "def total_backward_forward(X, Y, layers_dimensions, \n",
    "                           learning_rate, \n",
    "                           num_iterations, \n",
    "                           print_cost):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input:\n",
    "    X                 -- data\n",
    "    Y                 -- truth vector (1,0)'s\n",
    "    layers_dimensions -- list of dimensions for each layer of network\n",
    "    learning_rate     -- step size for gradient descent\n",
    "    num_iterations    -- number of training iterations to undertake\n",
    "    print_cost        -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    output:\n",
    "    parameters        -- parameters learnt by the model. Used to predict\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(42)\n",
    "    costs = []\n",
    "    \n",
    "    # Parameters initialization\n",
    "    parameters = initialise_parameters(layer_dimensions)\n",
    "    #print (\"y_train: \" + str(y_train.shape))\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation:\n",
    "        AL, caches = total_forward(X, parameters)\n",
    "        #print(AL)\n",
    "        \n",
    "        # Compute cost\n",
    "        cost = compute_cost(AL, Y)\n",
    "            \n",
    "        # Backward propagation.\n",
    "        grads = total_backward(AL, Y, caches)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1,10) and (1,3409) not aligned: 10 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-b3d8a65222c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m                                     \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                     \u001b[0mnum_iterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m                                     print_cost = True)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-8c4e35384bf8>\u001b[0m in \u001b[0;36mtotal_backward_forward\u001b[1;34m(X, Y, layers_dimensions, learning_rate, num_iterations, print_cost)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;31m# Backward propagation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m# Update parameters.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-531fbaa0905b>\u001b[0m in \u001b[0;36mtotal_backward\u001b[1;34m(AL, Y, caches)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mcurrent_cache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcaches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dA\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dW\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"db\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactivation_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_cache\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"sigmoid\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-ab323475234b>\u001b[0m in \u001b[0;36mactivation_backward\u001b[1;34m(dA, cache, activation)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"sigmoid\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mdZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation_cache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mdA_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlinear_cache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdA_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-62e79e12dae2>\u001b[0m in \u001b[0;36mlinear_backward\u001b[1;34m(dZ, cache)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mm\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m#dA_prev = np.dot(W.T,dZ)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mdA_prev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdA_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (1,10) and (1,3409) not aligned: 10 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "parameters = total_backward_forward(X_train, \n",
    "                                    y_train, \n",
    "                                    layer_dimensions, \n",
    "                                    learning_rate = 0.01,\n",
    "                                    num_iterations = 500, \n",
    "                                    print_cost = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict (Hold out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Create your own predict function.\n",
    "# Note the number of training examples\n",
    "# Turn the probabilities into 0-1 predictions\n",
    "# Replace False\n",
    "\n",
    "def predict(X, y, parameters):\n",
    "    \"\"\" \n",
    "    Input:\n",
    "    X           -- data (test set)\n",
    "    parameters  -- parameters of the trained model\n",
    "    \n",
    "    Output:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0] # How many training examples?\n",
    "    print('m: ' + str(m))\n",
    "    n = len(parameters) // 2\n",
    "    print('n: ' + str(n))\n",
    "    p = np.zeros((1,m)) # Initialise probabilities to zero\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = total_forward(X, parameters)\n",
    "    print('probas: ' + str(probas))\n",
    "    \n",
    "    # convert probas to 0/1 predictions. \n",
    "    p = np.where(probas >= 0.5,1,0)\n",
    "            \n",
    "    return p, probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some predictions\n",
    "predictions, probas = predict(X_test, y_test, parameters)\n",
    "# print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a scatter plot of probabilities. Good check if something is wrong\n",
    "plt.scatter(range(len(probas[0])), probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check your prediction value counts\n",
    "pred_df = pd.DataFrame(predictions[0], columns=[\"prediction\"])\n",
    "pred_df.prediction.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a bit of reshaping\n",
    "predictions_sk = predictions.reshape(len(predictions[0]), 1)\n",
    "print(predictions_sk.shape)\n",
    "\n",
    "y_test_sk = y_test.T\n",
    "print(y_test_sk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build some sklearn scores\n",
    "\n",
    "#Get confusion matrix \n",
    "print(\"Confustion Matrix \\n\", confusion_matrix(list(y_test_sk), list(predictions_sk)))\n",
    "\n",
    "#Get classification report\n",
    "print(classification_report(y_test_sk, predictions_sk))\n",
    "\n",
    "# Accuracy score\n",
    "print(\"Accuracy: \", accuracy_score(y_test_sk, predictions_sk))\n",
    "\n",
    "# ROC_AUC score\n",
    "print(\"ROC_AUC: \", roc_auc_score(y_test_sk, probas.T))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
